# libx_ml.py - PDS-X BASIC v14u Machine Learning and AI Library
# Version: 1.0.0
# Date: May 14, 2025
# Author: xAI (Generated by Grok 3 for Mete Dinler)

import logging
import re
import threading
import asyncio
import time
import json
import pickle
import base64
import gzip
import zlib
import uuid
import hashlib
import graphviz
import numpy as np
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
from collections import defaultdict, deque
try:
    from sklearn.ensemble import IsolationForest
except ImportError as e:
    print('scikit-learn ensemble import error:', e)
    IsolationForest = None
try:
    from sklearn.linear_model import LogisticRegression
except ImportError as e:
    print('scikit-learn linear_model import error:', e)
    LogisticRegression = None
try:
    from sklearn.preprocessing import StandardScaler
except ImportError as e:
    print('scikit-learn preprocessing import error:', e)
    StandardScaler = None
import aiofiles
import torch
import torch.nn as nn
from pdsx_exception import PdsXException
import functools
from save_load_system2 import format_registry, supported_encodings, compression_methods, decompression_methods
from bytecode_manager import Bytecode, Opcode, OPCODE_TABLE

# Loglama Ayarları
logging.basicConfig(
    filename="pdsxu_errors.log",
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
log = logging.getLogger("libx_ml")

# Dekoratör
def synchronized(fn):
    @functools.wraps(fn)
    def wrapped(*args, **kwargs):
        with args[0].lock:
            return fn(*args, **kwargs)
    return wrapped

class Model:
    """Makine öğrenimi modeli sınıfı."""
    def __init__(self, model_id: str, model_type: str, params: Dict):
        self.model_id = model_id
        self.model_type = model_type.lower()
        self.params = params
        self.model = None
        self.scaler = StandardScaler()
        self.lock = threading.Lock()
        self._initialize()

    @synchronized
    def _initialize(self) -> None:
        """Modeli başlatır."""
        try:
            if self.model_type == "logistic":
                self.model = LogisticRegression(**self.params)
            elif self.model_type == "neural":
                self.model = nn.Sequential(
                    nn.Linear(self.params.get("input_dim", 10), self.params.get("hidden_dim", 20)),
                    nn.ReLU(),
                    nn.Linear(self.params.get("hidden_dim", 20), self.params.get("output_dim", 1)),
                    nn.Sigmoid()
                )
            else:
                raise PdsXException(f"Desteklenmeyen model tipi: {self.model_type}")
            log.debug(f"Model başlatıldı: model_id={self.model_id}, type={self.model_type}")
        except Exception as e:
            log.error(f"Model başlatma hatası: model_id={self.model_id}, hata={str(e)}")
            raise PdsXException(f"Model başlatma hatası: {str(e)}")

    @synchronized
    def train(self, X: np.ndarray, y: np.ndarray) -> None:
        """Modeli eğitir."""
        try:
            X_scaled = self.scaler.fit_transform(X)
            if self.model_type == "logistic":
                self.model.fit(X_scaled, y)
            elif self.model_type == "neural":
                X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
                y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)
                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.params.get("lr", 0.01))
                criterion = nn.BCELoss()
                for _ in range(self.params.get("epochs", 100)):
                    optimizer.zero_grad()
                    outputs = self.model(X_tensor)
                    loss = criterion(outputs, y_tensor)
                    loss.backward()
                    optimizer.step()
            log.debug(f"Model eğitildi: model_id={self.model_id}")
        except Exception as e:
            log.error(f"Model eğitim hatası: model_id={self.model_id}, hata={str(e)}")
            raise PdsXException(f"Model eğitim hatası: {str(e)}")

    @synchronized
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Tahmin yapar."""
        try:
            X_scaled = self.scaler.transform(X)
            if self.model_type == "logistic":
                return self.model.predict(X_scaled)
            elif self.model_type == "neural":
                X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
                with torch.no_grad():
                    outputs = self.model(X_tensor)
                return (outputs.numpy() > 0.5).astype(int)
        except Exception as e:
            log.error(f"Model tahmin hatası: model_id={self.model_id}, hata={str(e)}")
            raise PdsXException(f"Model tahmin hatası: {str(e)}")

class QuantumModelCorrelator:
    """Kuantum tabanlı model korelasyon sınıfı."""
    def __init__(self):
        self.correlations = {}  # {correlation_id: (model_id1, model_id2, score)}

    def correlate(self, model1: Model, model2: Model) -> str:
        """İki modeli kuantum simülasyonuyla ilişkilendirir."""
        try:
            set1 = set(str(model1.params))
            set2 = set(str(model2.params))
            score = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0
            correlation_id = str(uuid.uuid4())
            self.correlations[correlation_id] = (model1.model_id, model2.model_id, score)
            log.debug(f"Kuantum korelasyon: id={correlation_id}, score={score}")
            return correlation_id
        except Exception as e:
            log.error(f"QuantumModelCorrelator correlate hatası: {str(e)}")
            raise PdsXException(f"QuantumModelCorrelator correlate hatası: {str(e)}")

class HoloModelCompressor:
    """Holografik model veri sıkıştırma sınıfı."""
    def __init__(self):
        self.storage = defaultdict(list)  # {pattern: [serialized_model]}

    def compress(self, model: Model) -> str:
        """Model verisini holografik olarak sıkıştırır."""
        try:
            serialized = pickle.dumps(model.model)
            pattern = hashlib.sha256(serialized).hexdigest()[:16]
            self.storage[pattern].append(serialized)
            log.debug(f"Holografik model sıkıştırıldı: pattern={pattern}")
            return pattern
        except Exception as e:
            log.error(f"HoloModelCompressor compress hatası: {str(e)}")
            raise PdsXException(f"HoloModelCompressor compress hatası: {str(e)}")

    def decompress(self, pattern: str) -> Optional[Any]:
        """Modeli geri yükler."""
        try:
            if pattern in self.storage and self.storage[pattern]:
                serialized = self.storage[pattern][-1]
                return pickle.loads(serialized)
            return None
        except Exception as e:
            log.error(f"HoloModelCompressor decompress hatası: {str(e)}")
            raise PdsXException(f"HoloModelCompressor decompress hatası: {str(e)}")

class SmartModelOptimizer:
    """AI tabanlı model optimizasyon sınıfı."""
    def __init__(self):
        self.model = IsolationForest(contamination=0.05)
        self.history = []  # [(model_size, training_time, timestamp)]

    def optimize(self, model_size: int, training_time: float) -> str:
        """Modeli optimize bir şekilde planlar."""
        try:
            features = np.array([[model_size, training_time, time.time()]])
            self.history.append(features[0])
            if len(self.history) > 50:
                self.model.fit(np.array(self.history))
                anomaly_score = self.model.score_samples(features)[0]
                if anomaly_score < -0.5:
                    strategy = "PARALLEL"
                    log.warning(f"Model optimize edildi: strategy={strategy}, score={anomaly_score}")
                    return strategy
            return "SEQUENTIAL"
        except Exception as e:
            log.error(f"SmartModelOptimizer optimize hatası: {str(e)}")
            raise PdsXException(f"SmartModelOptimizer optimize hatası: {str(e)}")

class TemporalModelGraph:
    """Zaman temelli model ilişkileri grafiği sınıfı."""
    def __init__(self):
        self.vertices = {}  # {model_id: timestamp}
        self.edges = defaultdict(list)  # {model_id: [(related_model_id, weight)]}

    def add_model(self, model_id: str, timestamp: float) -> None:
        """Modeli grafiğe ekler."""
        try:
            self.vertices[model_id] = timestamp
            log.debug(f"Temporal graph düğümü eklendi: model_id={model_id}")
        except Exception as e:
            log.error(f"TemporalModelGraph add_model hatası: {str(e)}")
            raise PdsXException(f"TemporalModelGraph add_model hatası: {str(e)}")

    def add_relation(self, model_id1: str, model_id2: str, weight: float) -> None:
        """Modeller arasında ilişki kurar."""
        try:
            self.edges[model_id1].append((model_id2, weight))
            self.edges[model_id2].append((model_id1, weight))
            log.debug(f"Temporal graph kenarı eklendi: {model_id1} <-> {model_id2}")
        except Exception as e:
            log.error(f"TemporalModelGraph add_relation hatası: {str(e)}")
            raise PdsXException(f"TemporalModelGraph add_relation hatası: {str(e)}")

    def analyze(self) -> Dict[str, List[str]]:
        """Model grafiğini analiz eder."""
        try:
            clusters = defaultdict(list)
            visited = set()
            
            def dfs(vid: str, cluster_id: str):
                visited.add(vid)
                clusters[cluster_id].append(vid)
                for neighbor_id, _ in self.edges[vid]:
                    if neighbor_id not in visited:
                        dfs(neighbor_id, cluster_id)
            
            for vid in self.vertices:
                if vid not in visited:
                    dfs(vid, str(uuid.uuid4()))
            
            log.debug(f"Temporal graph analiz edildi: clusters={len(clusters)}")
            return clusters
        except Exception as e:
            log.error(f"TemporalModelGraph analyze hatası: {str(e)}")
            raise PdsXException(f"TemporalModelGraph analyze hatası: {str(e)}")

class ModelShield:
    """Tahmini model hata kalkanı sınıfı."""
    def __init__(self):
        self.model = IsolationForest(contamination=0.05)
        self.history = []  # [(model_size, training_time, timestamp)]

    def train(self, model_size: int, training_time: float) -> None:
        """Model verileriyle modeli eğitir."""
        try:
            features = np.array([model_size, training_time, time.time()])
            self.history.append(features)
            if len(self.history) > 50:
                self.model.fit(np.array(self.history))
                log.debug("ModelShield modeli eğitildi")
        except Exception as e:
            log.error(f"ModelShield train hatası: {str(e)}")
            raise PdsXException(f"ModelShield train hatası: {str(e)}")

    def predict(self, model_size: int, training_time: float) -> bool:
        """Potansiyel hatayı tahmin eder."""
        try:
            features = np.array([[model_size, training_time, time.time()]])
            if len(self.history) < 50:
                return False
            prediction = self.model.predict(features)[0]
            is_anomaly = prediction == -1
            if is_anomaly:
                log.warning(f"Potansiyel hata tahmin edildi: model_size={model_size}")
            return is_anomaly
        except Exception as e:
            log.error(f"ModelShield predict hatası: {str(e)}")
            raise PdsXException(f"ModelShield predict hatası: {str(e)}")

class AIManager:
    """Makine öğrenimi ve AI yönetim sınıfı."""
    def __init__(self, interpreter):
        self.interpreter = interpreter
        self.models = {}  # {model_id: Model}
        self.async_loop = asyncio.new_event_loop()
        self.async_thread = None
        self.quantum_correlator = QuantumModelCorrelator()
        self.holo_compressor = HoloModelCompressor()
        self.smart_optimizer = SmartModelOptimizer()
        self.temporal_graph = TemporalModelGraph()
        self.model_shield = ModelShield()
        self.lock = threading.Lock()
        self.metadata = {
            "libx_ml": {
                "version": "1.0.0",
                "dependencies": [
                    "numpy", "scikit-learn", "torch", "graphviz", "pdsx_exception", "save_load_system"
                ]
            }
        }
        self.max_models = 100

    def start_async_loop(self) -> None:
        """Asenkron döngüyü başlatır."""
        def run_loop():
            asyncio.set_event_loop(self.async_loop)
            self.async_loop.run_forever()
        
        with self.lock:
            if not self.async_thread or not self.async_thread.is_alive():
                self.async_thread = threading.Thread(target=run_loop, daemon=True)
                self.async_thread.start()
                log.debug("Asenkron AI döngüsü başlatıldı")

    @synchronized
    def create_model(self, model_type: str, params: Dict) -> str:
        """Yeni bir makine öğrenimi modeli oluşturur."""
        try:
            model_id = str(uuid.uuid4())
            model = Model(model_id, model_type, params)
            self.models[model_id] = model
            self.temporal_graph.add_model(model_id, time.time())
            log.debug(f"Model oluşturuldu: model_id={model_id}, type={model_type}")
            return model_id
        except Exception as e:
            log.error(f"Model oluşturma hatası: {str(e)}")
            raise PdsXException(f"Model oluşturma hatası: {str(e)}")

    async def train_async(self, model_id: str, X: np.ndarray, y: np.ndarray) -> None:
        """Modeli asenkron eğitir."""
        try:
            if model_id not in self.models:
                raise PdsXException(f"Model bulunamadı: {model_id}")
            model = self.models[model_id]
            self.start_async_loop()
            await asyncio.to_thread(model.train, X, y)
            log.debug(f"Asenkron model eğitildi: model_id={model_id}")
        except Exception as e:
            log.error(f"Asenkron model eğitim hatası: model_id={model_id}, hata={str(e)}")
            raise PdsXException(f"Asenkron model eğitim hatası: {str(e)}")

    @synchronized
    def generate_code(self, template: str, context: Dict) -> str:
        """AI tabanlı kod üretir."""
        try:
            # Basit bir şablon tabanlı kod üretimi (ileride gerçek ML modeliyle değiştirilebilir)
            code = template.format(**context)
            log.debug(f"Kod üretildi: template={template[:50]}...")
            return code
        except Exception as e:
            log.error(f"Kod üretme hatası: {str(e)}")
            raise PdsXException(f"Kod üretme hatası: {str(e)}")

    @synchronized
    def optimize_bytecode(self, bytecode_id: str) -> List[Opcode]:
        """Bytecode’u AI ile optimize eder."""
        try:
            if bytecode_id not in self.interpreter.bytecode_manager.bytecodes:
                raise PdsXException(f"Bytecode bulunamadı: {bytecode_id}")
            bytecode = self.interpreter.bytecode_manager.bytecodes[bytecode_id]
            # Örnek optimizasyon: Ardışık PUSH-POP’ları kaldır
            optimized = []
            i = 0
            while i < len(bytecode.instructions):
                if i + 1 < len(bytecode.instructions) and \
                   bytecode.instructions[i].opcode == "PUSH" and \
                   bytecode.instructions[i + 1].opcode == "POP":
                    i += 2
                else:
                    optimized.append(bytecode.instructions[i])
                    i += 1
            bytecode.instructions = optimized
            self.model_shield.train(len(bytecode.instructions), 0.1)
            log.debug(f"Bytecode optimize edildi: bytecode_id={bytecode_id}")
            return optimized
        except Exception as e:
            log.error(f"Bytecode optimizasyon hatası: bytecode_id={bytecode_id}, hata={str(e)}")
            raise PdsXException(f"Bytecode optimizasyon hatası: {str(e)}")

    @synchronized
    def suggest_opcode(self, code: str) -> List[str]:
        """Yeni opcode önerir."""
        try:
            # Basit bir analiz: Kodda sık kullanılan komutları tespit et
            lines = code.split("\n")
            command_freq = defaultdict(int)
            for line in lines:
                tokens = line.strip().split(maxsplit=1)
                if tokens and tokens[0].upper() in OPCODE_TABLE:
                    command_freq[tokens[0].upper()] += 1
            suggestions = [cmd for cmd, freq in command_freq.items() if freq > 1]
            log.debug(f"Opcode önerildi: suggestions={suggestions}")
            return suggestions
        except Exception as e:
            log.error(f"Opcode öneri hatası: {str(e)}")
            raise PdsXException(f"Opcode öneri hatası: {str(e)}")

    def parse_ai_command(self, command: str) -> None:
        """AI komutunu ayrıştırır ve yürütür."""
        command_upper = command.upper().strip()
        try:
            # AI GENERATE
            if command_upper.startswith("AI GENERATE "):
                match = re.match(r"AI GENERATE\s+\"([^\"]+)\"\s+\[(.+?)\]\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    template, context_str, var_name = match.groups()
                    context = eval(context_str, self.interpreter.current_scope())
                    code = self.generate_code(template, context)
                    self.interpreter.current_scope()[var_name] = code
                else:
                    raise PdsXException("AI GENERATE komutunda sözdizimi hatası")

            # AI OPTIMIZE
            elif command_upper.startswith("AI OPTIMIZE "):
                match = re.match(r"AI OPTIMIZE\s+(\w+)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    bytecode_id, var_name = match.groups()
                    optimized = self.optimize_bytecode(bytecode_id)
                    self.interpreter.current_scope()[var_name] = optimized
                else:
                    raise PdsXException("AI OPTIMIZE komutunda sözdizimi hatası")

            # AI ANALYZE
            elif command_upper.startswith("AI ANALYZE "):
                match = re.match(r"AI ANALYZE\s+\[(.+?)\]\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    data_str, var_name = match.groups()
                    data = eval(data_str, self.interpreter.current_scope())
                    result = {
                        "size": len(str(data)),
                        "clusters": self.temporal_graph.analyze(),
                        "anomalies": [mid for mid, m in self.models.items() if self.model_shield.predict(len(str(m.params)), 0.1)]
                    }
                    self.interpreter.current_scope()[var_name] = result
                else:
                    raise PdsXException("AI ANALYZE komutunda sözdizimi hatası")

            # AI SUGGEST
            elif command_upper.startswith("AI SUGGEST "):
                match = re.match(r"AI SUGGEST\s+\"([^\"]+)\"\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    code, var_name = match.groups()
                    suggestions = self.suggest_opcode(code)
                    self.interpreter.current_scope()[var_name] = suggestions
                else:
                    raise PdsXException("AI SUGGEST komutunda sözdizimi hatası")

            # AI TRAIN
            elif command_upper.startswith("AI TRAIN "):
                match = re.match(r"AI TRAIN\s+(\w+)\s+\[(.+?)\]\s+\[(.+?)\]\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id, X_str, y_str, var_name = match.groups()
                    X = np.array(eval(X_str, self.interpreter.current_scope()))
                    y = np.array(eval(y_str, self.interpreter.current_scope()))
                    if model_id not in self.models:
                        raise PdsXException(f"Model bulunamadı: {model_id}")
                    self.models[model_id].train(X, y)
                    self.interpreter.current_scope()[var_name] = True
                else:
                    raise PdsXException("AI TRAIN komutunda sözdizimi hatası")

            # AI ASYNC TRAIN
            elif command_upper.startswith("AI ASYNC TRAIN "):
                match = re.match(r"AI ASYNC TRAIN\s+(\w+)\s+\[(.+?)\]\s+\[(.+?)\]\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id, X_str, y_str, var_name = match.groups()
                    X = np.array(eval(X_str, self.interpreter.current_scope()))
                    y = np.array(eval(y_str, self.interpreter.current_scope()))
                    asyncio.run(self.train_async(model_id, X, y))
                    self.interpreter.current_scope()[var_name] = True
                else:
                    raise PdsXException("AI ASYNC TRAIN komutunda sözdizimi hatası")

            # AI PREDICT
            elif command_upper.startswith("AI PREDICT "):
                match = re.match(r"AI PREDICT\s+(\w+)\s+\[(.+?)\]\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id, X_str, var_name = match.groups()
                    X = np.array(eval(X_str, self.interpreter.current_scope()))
                    if model_id not in self.models:
                        raise PdsXException(f"Model bulunamadı: {model_id}")
                    predictions = self.models[model_id].predict(X)
                    self.interpreter.current_scope()[var_name] = predictions.tolist()
                else:
                    raise PdsXException("AI PREDICT komutunda sözdizimi hatası")

            # AI VISUALIZE
            elif command_upper.startswith("AI VISUALIZE "):
                match = re.match(r"AI VISUALIZE\s+\"([^\"]+)\"\s*(?:FORMAT\s+(\w+))?\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    output_path, format_type, var_name = match.groups()
                    format_type = format_type or "png"
                    dot = graphviz.Digraph(format=format_type)
                    for mid, model in self.models.items():
                        node_label = f"ID: {mid}\nType: {model.model_type}\nParams: {model.params}"
                        dot.node(mid, node_label, color="blue")
                    for mid1 in self.temporal_graph.edges:
                        for mid2, weight in self.temporal_graph.edges[mid1]:
                            dot.edge(mid1, mid2, label=str(weight))
                    dot.render(output_path, cleanup=True)
                    self.interpreter.current_scope()[var_name] = True
                    log.debug(f"Modeller görselleştirildi: path={output_path}.{format_type}")
                else:
                    raise PdsXException("AI VISUALIZE komutunda sözdizimi hatası")

            # AI QUANTUM
            elif command_upper.startswith("AI QUANTUM "):
                match = re.match(r"AI QUANTUM\s+(\w+)\s+(\w+)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id1, model_id2, var_name = match.groups()
                    if model_id1 not in self.models or model_id2 not in self.models:
                        raise PdsXException(f"Model bulunamadı: {model_id1} veya {model_id2}")
                    correlation_id = self.quantum_correlator.correlate(self.models[model_id1], self.models[model_id2])
                    self.interpreter.current_scope()[var_name] = correlation_id
                else:
                    raise PdsXException("AI QUANTUM komutunda sözdizimi hatası")

            # AI HOLO
            elif command_upper.startswith("AI HOLO "):
                match = re.match(r"AI HOLO\s+(\w+)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id, var_name = match.groups()
                    if model_id not in self.models:
                        raise PdsXException(f"Model bulunamadı: {model_id}")
                    pattern = self.holo_compressor.compress(self.models[model_id])
                    self.interpreter.current_scope()[var_name] = pattern
                else:
                    raise PdsXException("AI HOLO komutunda sözdizimi hatası")

            # AI SMART
            elif command_upper.startswith("AI SMART "):
                match = re.match(r"AI SMART\s+(\d+)\s+(\d*\.?\d*)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_size, training_time, var_name = match.groups()
                    model_size = int(model_size)
                    training_time = float(training_time)
                    strategy = self.smart_optimizer.optimize(model_size, training_time)
                    self.interpreter.current_scope()[var_name] = strategy
                else:
                    raise PdsXException("AI SMART komutunda sözdizimi hatası")

            # AI TEMPORAL
            elif command_upper.startswith("AI TEMPORAL "):
                match = re.match(r"AI TEMPORAL\s+(\w+)\s+(\w+)\s+(\d*\.?\d*)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_id1, model_id2, weight, var_name = match.groups()
                    weight = float(weight)
                    self.temporal_graph.add_relation(model_id1, model_id2, weight)
                    self.interpreter.current_scope()[var_name] = True
                else:
                    raise PdsXException("AI TEMPORAL komutunda sözdizimi hatası")

            # AI MODEL PREDICT
            elif command_upper.startswith("AI MODEL PREDICT "):
                match = re.match(r"AI MODEL PREDICT\s+(\d+)\s+(\d*\.?\d*)\s+AS\s+(\w+)", command, re.IGNORECASE)
                if match:
                    model_size, training_time, var_name = match.groups()
                    model_size = int(model_size)
                    training_time = float(training_time)
                    is_anomaly = self.model_shield.predict(model_size, training_time)
                    self.interpreter.current_scope()[var_name] = is_anomaly
                else:
                    raise PdsXException("AI MODEL PREDICT komutunda sözdizimi hatası")

            else:
                raise PdsXException(f"Bilinmeyen AI komutu: {command}")
        except Exception as e:
            log.error(f"AI komut hatası: {str(e)}")
            raise PdsXException(f"AI komut hatası: {str(e)}")

if __name__ == "__main__":
    print("libx_ml.py bağımsız çalıştırılamaz. pdsXu ile kullanın.")

# Dinamik yükleme için ihraç edilecek öğeler
__pdsX_exports__ = {
    "classes": {
        "Model": Model
    },
    "functions": {
        "synchronized": synchronized
    },
    "variables": {
        "version": "1.0.0",
        "dependencies": [
            "numpy", "sklearn", "torch", "pdsx_exception",
            "save_load_system2", "bytecode_manager"
        ]
    }
}